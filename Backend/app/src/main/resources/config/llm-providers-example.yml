activeProvider: ollama
providers:
  - id: openrouter
    displayName: OpenRouter Community
    mode: remote
    baseUrl: https://openrouter.ai/api/v1
    apiKey: "${LLM_OPENROUTER_API_KEY:}"
    model: anthropic/claude-3-haiku
    enabled: true
    metadata:
      notes: >
        Provide an API key via LLM_OPENROUTER_API_KEY env variable if you want to rely on the hosted model.

  - id: ollama
    displayName: Ollama Local (RTX 3070 8 GB)
    mode: local
    baseUrl: http://localhost:11434
    model: llama3.2:3b
    enabled: true
    metadata:
      minGpuMemory: 8GB
      recommendedCommand: |
        ollama pull llama3.2:3b
        ollama run llama3.2:3b --keepalive 5m
