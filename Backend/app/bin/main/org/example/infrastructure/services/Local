package org.example.infrastructure.services;

import com.microsoft.onnxruntime.OrtEnvironment;
import com.microsoft.onnxruntime.OrtSession;
import org.example.domain.entities.LLMModel;
import org.example.domain.entities.LLMProvider;
import org.example.domain.valueobjects.ModelStatus;
import org.example.infrastructure.config.LLMConfig;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;
import org.zeroturnaround.exec.ProcessExecutor;
import org.zeroturnaround.exec.ProcessResult;

import java.io.File;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

/**
 * Local LLM service providing ONNX model loading and process management
 * Replicates the local model functionality from ScriptRating
 */
@Service
public class LocalLLMService {
    
    private static final Logger logger = LoggerFactory.getLogger(LocalLLMService.class);
    
    private final LLMConfig config;
    private final Map<String, LoadedModel> loadedModels = new ConcurrentHashMap<>();
    private final Map<String, ModelInfo> availableModels = new ConcurrentHashMap<>();
    private final ScheduledExecutorService monitoringExecutor = Executors.newScheduledThreadPool(2);
    private OrtEnvironment ortEnvironment;
    
    public LocalLLMService(LLMConfig config) {
        this.config = config;
        initializeAvailableModels();
        startMonitoring();
    }
    
    // Inner classes for model management
    public static class ModelInfo {
        private final String modelName;
        private final String filePath;
        private final Double sizeGB;
        private final String description;
        
        public ModelInfo(String modelName, String filePath, Double sizeGB, String description) {
            this.modelName = modelName;
            this.filePath = filePath;
            this.sizeGB = sizeGB;
            this.description = description;
        }
        
        public String getModelName() { return modelName; }
        public String getFilePath() { return filePath; }
        public Double getSizeGB() { return sizeGB; }
        public String getDescription() { return description; }
    }
    
    public static class LoadedModel {
        private final String modelName;
        private final OrtSession session;
        private final long loadTime;
        private final String modelPath;
        private volatile boolean healthy;
        private volatile long lastUsed;
        
        public LoadedModel(String modelName, OrtSession session, String modelPath) {
            this.modelName = modelName;
            this.session = session;
            this.modelPath = modelPath;
            this.loadTime = System.currentTimeMillis();
            this.healthy = true;
            this.lastUsed = System.currentTimeMillis();
        }
        
        public String getModelName() { return modelName; }
        public OrtSession getSession() { return session; }
        public long getLoadTime() { return loadTime; }
        public String getModelPath() { return modelPath; }
        public boolean isHealthy() { return healthy; }
        public void setHealthy(boolean healthy) { this.healthy = healthy; }
        public long getLastUsed() { return lastUsed; }
        public void updateLastUsed() { this.lastUsed = System.currentTimeMillis(); }
    }
    
    // Model management methods
    public List<ModelInfo> getAvailableModels() {
        return new ArrayList<>(availableModels.values());
    }
    
    public List<String> getLoadedModelNames() {
        return new ArrayList<>(loadedModels.keySet());
    }
    
    public boolean isModelLoaded(String modelName) {
        return loadedModels.containsKey(modelName);
    }
    
    public Optional<LoadedModel> getLoadedModel(String modelName) {
        return Optional.ofNullable(loadedModels.get(modelName));
    }
    
    public ModelInfo getModelInfo(String modelName) {
        return availableModels.get(modelName);
    }
    
    // Model loading/unloading
    public boolean loadModel(String modelName) {
        if (isModelLoaded(modelName)) {
            logger.warn("Model {} is already loaded", modelName);
            return true;
        }
        
        ModelInfo modelInfo = availableModels.get(modelName);
        if (modelInfo == null) {
            logger.error("Model {} not found in available models", modelName);
            return false;
        }
        
        if (loadedModels.size() >= config.getMaxLocalModels()) {
            logger.warn("Maximum number of local models ({}) reached", config.getMaxLocalModels());
            return false;
        }
        
        try {
            logger.info("Loading model: {}", modelName);
            
            // Initialize ONNX environment if not already done
            if (ortEnvironment == null) {
                ortEnvironment = OrtEnvironment.getEnvironment();
            }
            
            // Load the ONNX model
            OrtSession session = ortEnvironment.createSession(modelInfo.getFilePath(), new OrtSession.SessionOptions());
            
            LoadedModel loadedModel = new LoadedModel(modelName, session, modelInfo.getFilePath());
            loadedModels.put(modelName, loadedModel);
            
            logger.info("Successfully loaded model: {} (path: {})", modelName, modelInfo.getFilePath());
            return true;
            
        } catch (Exception e) {
            logger.error("Failed to load model: {}", modelName, e);
            return false;
        }
    }
    
    public boolean unloadModel(String modelName) {
        LoadedModel loadedModel = loadedModels.get(modelName);
        if (loadedModel == null) {
            logger.warn("Model {} is not loaded", modelName);
            return false;
        }
        
        try {
            logger.info("Unloading model: {}", modelName);
            
            // Close the ONNX session
            loadedModel.getSession().close();
            
            // Remove from loaded models
            loadedModels.remove(modelName);
            
            logger.info("Successfully unloaded model: {}", modelName);
            return true;
            
        } catch (Exception e) {
            logger.error("Failed to unload model: {}", modelName, e);
            return false;
        }
    }
    
    // Process management for external local LLM servers (like Ollama)
    public boolean startLocalServer(String serverType, int port) {
        try {
            logger.info("Starting local {} server on port {}", serverType, port);
            
            ProcessExecutor processExecutor = new ProcessExecutor()
                .command(getServerCommand(serverType))
                .directory(new File("."))
                .redirectErrorStream(true);
            
            ProcessResult result = processExecutor.execute();
            
            if (result.getExitCode() == 0) {
                logger.info("Successfully started {} server", serverType);
                return true;
            } else {
                logger.error("Failed to start {} server. Exit code: {}", serverType, result.getExitCode());
                return false;
            }
            
        } catch (Exception e) {
            logger.error("Exception starting {} server", serverType, e);
            return false;
        }
    }
    
    public boolean stopLocalServer(String serverType) {
        try {
            logger.info("Stopping local {} server", serverType);
            
            // Implementation would depend on how we track server processes
            // This is a simplified version
            ProcessExecutor processExecutor = new ProcessExecutor()
                .command("pkill", "-f", getServerProcessName(serverType));
            
            ProcessResult result = processExecutor.execute();
            
            logger.info("Stopped {} server", serverType);
            return true;
            
        } catch (Exception e) {
            logger.error("Exception stopping {} server", serverType, e);
            return false;
        }
    }
    
    public boolean isLocalServerRunning(String serverType) {
        try {
            ProcessExecutor processExecutor = new ProcessExecutor()
                .command("pgrep", "-f", getServerProcessName(serverType));
            
            ProcessResult result = processExecutor.execute();
            return result.getExitCode() == 0;
            
        } catch (Exception e) {
            logger.error("Error checking if {} server is running", serverType, e);
            return false;
        }
    }
    
    // Health monitoring
    public boolean isModelHealthy(String modelName) {
        LoadedModel loadedModel = loadedModels.get(modelName);
        return loadedModel != null && loadedModel.isHealthy();
    }
    
    public void markModelAsUnhealthy(String modelName) {
        LoadedModel loadedModel = loadedModels.get(modelName);
        if (loadedModel != null) {
            loadedModel.setHealthy(false);
            logger.warn("Marked model {} as unhealthy", modelName);
        }
    }
    
    public double getModelLoadTime(String modelName) {
        LoadedModel loadedModel = loadedModels.get(modelName);
        if (loadedModel == null) {
            return -1;
        }
        return (System.currentTimeMillis() - loadedModel.getLoadTime()) / 1000.0;
    }
    
    // Inference methods
    public Optional<String> generateText(String modelName, String prompt, int maxTokens) {
        LoadedModel loadedModel = loadedModels.get(modelName);
        if (loadedModel == null) {
            logger.warn("Model {} is not loaded", modelName);
            return Optional.empty();
        }
        
        try {
            loadedModel.updateLastUsed();
            
            // TODO: Implement actual ONNX inference
            // This would involve:
            // 1. Tokenizing the input prompt
            // 2. Running inference on the ONNX model
            // 3. Decoding the output tokens to text
            
            // For now, return a mock response
            String mockResponse = "Mock inference result for: " + prompt.substring(0, Math.min(50, prompt.length()));
            return Optional.of(mockResponse);
            
        } catch (Exception e) {
            logger.error("Error during inference for model {}", modelName, e);
            markModelAsUnhealthy(modelName);
            return Optional.empty();
        }
    }
    
    // Utility methods
    private void initializeAvailableModels() {
        // Initialize available models from configuration
        String modelPath = config.getLocalModelPath();
        
        // Add default models if they exist
        String[] defaultModels = {
            "llama2-7b.onnx", "mistral-7b.onnx", "llama2-13b.onnx"
        };
        
        for (String modelFile : defaultModels) {
            Path fullPath = Paths.get(modelPath, modelFile);
            if (fullPath.toFile().exists()) {
                ModelInfo modelInfo = new ModelInfo(
                    modelFile.replace(".onnx", ""),
                    fullPath.toString(),
                    3.9, // Default size
                    "Local ONNX model"
                );
                availableModels.put(modelInfo.getModelName(), modelInfo);
            }
        }
        
        logger.info("Initialized {} available local models", availableModels.size());
    }
    
    private void startMonitoring() {
        // Monitor loaded models health every 30 seconds
        monitoringExecutor.scheduleAtFixedRate(this::monitorLoadedModels, 30, 30, TimeUnit.SECONDS);
        
        // Clean up unused models every 5 minutes
        monitoringExecutor.scheduleAtFixedRate(this::cleanupUnusedModels, 5, 5, TimeUnit.MINUTES);
    }
    
    private void monitorLoadedModels() {
        logger.debug("Monitoring {} loaded models", loadedModels.size());
        
        for (LoadedModel model : loadedModels.values()) {
            try {
                // Check if model session is still valid
                if (model.getSession() == null || model.getSession().isClosed()) {
                    logger.warn("Model {} session is closed, marking as unhealthy", model.getModelName());
                    model.setHealthy(false);
                }
            } catch (Exception e) {
                logger.error("Error monitoring model {}", model.getModelName(), e);
                model.setHealthy(false);
            }
        }
    }
    
    private void cleanupUnusedModels() {
        long currentTime = System.currentTimeMillis();
        long unusedThreshold = 30 * 60 * 1000; // 30 minutes
        
        List<String> toUnload = new ArrayList<>();
        
        for (LoadedModel model : loadedModels.values()) {
            if (currentTime - model.getLastUsed() > unusedThreshold) {
                logger.info("Model {} has been unused for {} minutes, marking for cleanup", 
                           model.getModelName(), (currentTime - model.getLastUsed()) / (60 * 1000));
                toUnload.add(model.getModelName());
            }
        }
        
        for (String modelName : toUnload) {
            unloadModel(modelName);
        }
    }
    
    private List<String> getServerCommand(String serverType) {
        switch (serverType.toLowerCase()) {
            case "ollama":
                return Arrays.asList("ollama", "serve");
            case "lm-studio":
                return Arrays.asList("lm-studio", "--port", String.valueOf(config.getLocalServerPort()));
            default:
                throw new IllegalArgumentException("Unknown server type: " + serverType);
        }
    }
    
    private String getServerProcessName(String serverType) {
        switch (serverType.toLowerCase()) {
            case "ollama":
                return "ollama serve";
            case "lm-studio":
                return "lm-studio";
            default:
                return serverType;
        }
    }
    
    // Cleanup
    public void shutdown() {
        logger.info("Shutting down Local LLM Service");
        
        // Stop monitoring
        monitoringExecutor.shutdown();
        try {
            monitoringExecutor.awaitTermination(5, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            logger.warn("Interrupted during shutdown");
        }
        
        // Unload all models
        for (String modelName : new ArrayList<>(loadedModels.keySet())) {
            unloadModel(modelName);
        }
        
        // Close ONNX environment
        if (ortEnvironment != null) {
            ortEnvironment.close();
        }
        
        logger.info("Local LLM Service shutdown complete");
    }
}